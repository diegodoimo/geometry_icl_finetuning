#!/bin/bash
#SBATCH --partition=DGX
#SBATCH --account=lade
#SBATCH --nodes=1
#SBATCH --time=2:00:00            
#SBATCH --ntasks-per-node=1       
#SBATCH --cpus-per-task=32           
#SBATCH --mem=100G                
#SBATCH --job-name=test
#SBATCH --gres=gpu:1


source /u/area/ddoimo/anaconda3/bin/activate ./env_emd
export OMP_NUM_THREADS=32
export PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True"

model_name=llama-3-8b
path=llama_v3


torchrun --rdzv-backend=c10d --rdzv-endpoint=localhost:0 \
       	--nnodes=1  --nproc-per-node=1 \
    extract_repr.py \
    --checkpoint_dir  "/u/area/ddoimo/ddoimo/models/$path/models_hf/$model_name" \
    --use_slow_tokenizer \
    --preprocessing_num_workers 16 \
    --micro_batch_size 1 \
    --out_dir "./results" \
    --logging_steps 100 \
    --layer_interval 1 \
    --remove_duplicates \
    --use_last_token \
    --max_seq_len 4090 \
    --split "test" \
    --num_few_shots 5 \
    --save_distances \
    --save_repr \
    --dataset_name "scienceqa" \
    --dataset_path "/orfeo/cephfs/scratch/area/ddoimo/open/geometric_lens/repo/diego/science_qa"
    #--seed $seed \
    #--finetuned_path  "/u/area/ddoimo/ddoimo/finetuning_llm/open-instruct/results" \
    #--finetuned_mode "dev_val_balanced_20samples" \
    #--finetuned_epochs 4 \
    #--model_seed 1 \
    #--save_distances \
    #--save_repr 
