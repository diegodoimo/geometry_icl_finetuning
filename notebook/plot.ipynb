{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from src.metrics.intrinsic_dimension import IntrinsicDimension\n",
    "from src.metrics.clustering import LabelClustering\n",
    "from src.utils.tensor_storage import retrieve_from_storage\n",
    "import numpy as np\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import pandas as pd\n",
    "plot_config = {\n",
    "    'axes.titlesize': 30,      \n",
    "    'axes.labelsize': 29,\n",
    "    'xtick.labelsize': 20,\n",
    "    'ytick.labelsize': 20,\n",
    "    'legend.fontsize': 23,\n",
    "    'figure.figsize': (10, 8),\n",
    "    'lines.linewidth': 2.5,\n",
    "    'lines.markersize': 10,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistral-1-7b\"\n",
    "\n",
    "title = model_name\n",
    "title = title.replace(\"-\", \" \")\n",
    "title = title[0].upper() + title[1:]\n",
    "\n",
    "_PATH_ft = Path(f\"/orfeo/cephfs/scratch/area/ddoimo/open\"\\\n",
    "                f\"/geometric_lens/repo/results\"\\\n",
    "                f\"/finetuned_dev_val_balanced_40samples\"\\\n",
    "                f\"/evaluated_test/{model_name}/6epochs/epoch_6\")\n",
    "\n",
    "_PATH = Path(f\"/orfeo/cephfs/scratch/area/ddoimo/open/geometric_lens\"\n",
    "             f\"/repo/results/evaluated_test/random_order/{model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_num_shot(path):\n",
    "    if \"70b\" in path:\n",
    "        return 4\n",
    "    else:\n",
    "        return 5\n",
    "\n",
    "def average_custom_blocks(input, window):\n",
    "    \"\"\"\n",
    "    For the plots in the main section of the paper we average the value of a certain metric for a specific layer\n",
    "    over a window of n layers. This is done in order to smooth the profile.\n",
    "    \"\"\"\n",
    "    if window == 0:\n",
    "        return input\n",
    "    input_avg = []\n",
    "    input_avg.append(np.mean(input_avg[0:window]))\n",
    "    if len(input_avg) > window:\n",
    "        input_avg.append(np.mean(input_avg[0:window+1]))\n",
    "\n",
    "    for i in range(1, len(input_avg)-1):\n",
    "        input_avg.append(np.mean(input_avg[i:window+i+1]))\n",
    "    assert len(input_avg) == len(input), f\"y_avg:{len(input_avg)}, y:{len(y)}\"\n",
    "\n",
    "    return np.array(input_avg)\n",
    "\n",
    "\n",
    "def plotter(file_name,\n",
    "            model,\n",
    "            data,\n",
    "            title,\n",
    "            ylabel,\n",
    "            yticks=None,\n",
    "            avg=0):\n",
    "    \n",
    "    # Set the style\n",
    "    sns.set_style(\n",
    "        \"whitegrid\",\n",
    "        rc={\"axes.edgecolor\": \".15\", \"xtick.bottom\": True, \"ytick.left\": True},\n",
    "    )\n",
    "    # Setup figure and axes for 2 plots in one row\n",
    "    plt.figure(dpi=200)\n",
    "    layers = np.arange(0, data[0].shape[0])\n",
    "\n",
    "    # Set ticks\n",
    "    if layers.shape[0] < 50:\n",
    "        # Generates positions 0, 4, 8, ...\n",
    "        tick_positions = np.arange(0, layers.shape[0], 4)  \n",
    "    else:\n",
    "        # Generates positions 0, 4, 8, ...\n",
    "        tick_positions = np.arange(0, layers.shape[0], 8)  \n",
    "\n",
    "    tick_labels = tick_positions + 1  # Get the corresponding labels from x\n",
    "\n",
    "    names = [\"0 shot pt\",\n",
    "             \"1 shot pt\",\n",
    "             \"2 shot pt\",\n",
    "             \"5 shot pt\",\n",
    "             \"0 shot ft\"]\n",
    "    markerstyle = ['o', 'o', 'o', 'o', 'x']\n",
    "   \n",
    "    for int_dim, label, markerstyle in zip(data, names, markerstyle):\n",
    "        int_dim = average_custom_blocks(int_dim, avg)\n",
    "        sns.scatterplot(x=layers, y=int_dim, marker=markerstyle)\n",
    "        sns.lineplot(x=layers, y=int_dim, label=label)\n",
    "\n",
    "    plt.xlabel(\"Layer\")\n",
    "    plt.ylabel(ylabel)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    if yticks:\n",
    "        plt.xticks(ticks=tick_positions, labels=tick_labels)\n",
    "        if isinstance(yticks, list):\n",
    "            tick_positions_y = np.arange(yticks[0], (yticks[1] + (yticks[1]-yticks[0])/10),\n",
    "                                         (yticks[1]-yticks[0])/10).round(2)\n",
    "        else:\n",
    "            tick_positions_y = np.arange(0, (yticks + yticks/10), yticks/10).round(2)\n",
    "        plt.yticks(tick_positions_y)\n",
    "    plt.tick_params(axis='y')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.rcParams.update(plot_config)\n",
    "\n",
    "    # setting fi\n",
    "    file_name = file_name.replace(\" \", \"_\")\n",
    "    if ylabel == \"ARI\":\n",
    "        file_name = file_name + \"_1.6\"\n",
    "    # file_name = file_name + \"_\"+model\n",
    "    file_name = file_name + \"_avg_\" + str(avg) if avg > 0 else file_name    \n",
    "    file_name = file_name + \"_no_title\" if not title else file_name\n",
    "    if title:\n",
    "        path = Path(f\"plots/{file_name}.pdf\")\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        # plt.savefig(f\"plots/{file_name}.png\")\n",
    "        plt.savefig(path, format='pdf')\n",
    "    else:\n",
    "        path = Path(f\"plots/no_title/{file_name}.pdf\")\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        # plt.savefig(f\"plots/no_title/{file_name}.png\")\n",
    "        plt.savefig(path, format='pdf')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of the window for the average\n",
    "AVG = 2\n",
    "\n",
    "# List of metrics used in the paper\n",
    "_METRICS = [\"intrinsic_dimension\",\n",
    "            \"clustering_subject\",\n",
    "            \"clustering_letter\",\n",
    "            \"clusters_analysis\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper results \n",
    "The following snippet can be used to replicate some of the plot from the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intrinsic Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"intrinsic_dimension\" in _METRICS:\n",
    "    print(\"Intrinsic Dimension\")\n",
    "\n",
    "    shot = [0, 1, 2, find_num_shot(str(_PATH))]\n",
    "    data = []\n",
    "\n",
    "    cache_path = Path(f\"cache/{model_name}/\")\n",
    "    cache_path.mkdir(parents=True, exist_ok=True)\n",
    "    if os.path.exists(cache_path / \"intrinsic_dim.pkl\"):\n",
    "        print(\"Loading from cache\")\n",
    "        with open(cache_path / \"intrinsic_dim.pkl\", \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "    else:\n",
    "        for i in shot:\n",
    "            out_from_storage = retrieve_from_storage(_PATH / f'{i}shot',\n",
    "                                                     full_tensor=True,\n",
    "                                                     instances_per_sub=200)\n",
    "            tensors, _, number_of_layers = out_from_storage\n",
    "            intrinsic_dim = IntrinsicDimension()\n",
    "            \n",
    "            data.append(intrinsic_dim.main(tensors, number_of_layers))\n",
    "\n",
    "        out_from_storage = retrieve_from_storage(_PATH_ft,\n",
    "                                                 full_tensor=True,\n",
    "                                                 instances_per_sub=200)\n",
    "        tensors, _, number_of_layers = out_from_storage\n",
    "        intrinsic_dim = IntrinsicDimension()\n",
    "        data.append(intrinsic_dim.main(tensors, number_of_layers))\n",
    "        with open(cache_path / \"intrinsic_dim.pkl\", \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "    \n",
    "    # Selecting the order of nearest neighbors considered in gride\n",
    "    data_nn_index = [arr[:, -3] for arr in data]\n",
    "    plotter(file_name=\"ID/\"+model_name, model=model_name,\n",
    "            data=data_nn_index, title=title, ylabel=\"ID\", avg=0, yticks=22)\n",
    "    plotter(file_name=\"ID/\"+model_name, model=model_name,\n",
    "            data=data_nn_index, title=title, ylabel=\"ID\", avg=AVG, yticks=22)\n",
    "    print(\"Intrinsic Dimension done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"clustering_subject\" in _METRICS:\n",
    "    print(\"Clustering Subject\")\n",
    "    shot = [0, 1, 2, find_num_shot(str(_PATH))]\n",
    "    data_subjects = []\n",
    "\n",
    "    cache_path = Path(f\"cache/{model_name}/\")\n",
    "    cache_path.mkdir(parents=True, exist_ok=True)\n",
    "    if os.path.exists(cache_path / \"subject.pkl\"):\n",
    "        print(\"Loading from cache\")\n",
    "        with open(cache_path / \"subject.pkl\", \"rb\") as f:\n",
    "            data_subjects = pickle.load(f)\n",
    "    else:\n",
    "        for i in shot:\n",
    "            clustering = LabelClustering()\n",
    "            out_from_storage = retrieve_from_storage(_PATH / f'{i}shot',\n",
    "                                                     full_tensor=True,\n",
    "                                                     instances_per_sub=200)\n",
    "            tensors, labels, number_of_layers = out_from_storage\n",
    "            data_subjects.append(clustering.main(\n",
    "                z=1.68,\n",
    "                tensors=tensors,\n",
    "                labels=labels[\"subjects\"],\n",
    "                number_of_layers=number_of_layers\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        clustering = LabelClustering()\n",
    "        out_from_storage = retrieve_from_storage(_PATH_ft,\n",
    "                                                 full_tensor=True,\n",
    "                                                 instances_per_sub=200)\n",
    "        tensors, labels, number_of_layers = out_from_storage\n",
    "        data_subjects.append(clustering.main(z=1.68,\n",
    "                                             tensors=tensors,\n",
    "                                             labels=labels[\"subjects\"],\n",
    "                                             number_of_layers=number_of_layers))\n",
    "\n",
    "        with open(cache_path / \"subject.pkl\", \"wb\") as f:\n",
    "            pickle.dump(data_subjects, f)\n",
    "\n",
    "    ari = [np.array(i['adjusted_rand_score']) for i in data_subjects]\n",
    "\n",
    "    plotter(file_name=\"ari_subjects/\"+model_name, model=model_name, data=ari, title=title,\n",
    "            ylabel=\"ARI\", avg=0, yticks=0.9)\n",
    "    plotter(file_name=\"ari_subjects/\"+model_name, model=model_name, data=ari, title=title,\n",
    "            ylabel=\"ARI\", avg=AVG, yticks=0.9)\n",
    "    print(\"Clustering Subject done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"clustering_letter\" in _METRICS:\n",
    "    print(\"Clustering Letters\")\n",
    "    shot = [0, 1, 2, find_num_shot(str(_PATH))]\n",
    "    data_letter = []\n",
    "\n",
    "    cache_path = Path(f\"cache/{model_name}/\")\n",
    "    cache_path.mkdir(parents=True, exist_ok=True)\n",
    "    if os.path.exists(cache_path / \"letter.pkl\"):\n",
    "        print(\"Loading from cache\")\n",
    "        with open(cache_path / \"letter.pkl\", \"rb\") as f:\n",
    "            data_letter = pickle.load(f)\n",
    "    else:\n",
    "        for i in shot:\n",
    "            clustering = LabelClustering()\n",
    "            out_from_storage = retrieve_from_storage(_PATH / f'{i}shot',\n",
    "                                                     full_tensor=True,\n",
    "                                                     instances_per_sub=200)\n",
    "            tensors, labels, number_of_layers = out_from_storage\n",
    "            data_letter.append(clustering.main(\n",
    "                z=1.68,\n",
    "                tensors=tensors,\n",
    "                labels=labels[\"predictions\"],\n",
    "                number_of_layers=number_of_layers\n",
    "                )\n",
    "            )\n",
    "                \n",
    "        clustering = LabelClustering()\n",
    "        out_from_storage = retrieve_from_storage(_PATH_ft,\n",
    "                                                 full_tensor=True,\n",
    "                                                 instances_per_sub=200)\n",
    "        tensors, labels, number_of_layers = out_from_storage\n",
    "        data_letter.append(clustering.main(\n",
    "            z=1.68,\n",
    "            tensors=tensors,\n",
    "            labels=labels[\"predictions\"],\n",
    "            number_of_layers=number_of_layers\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        with open(cache_path / \"letter.pkl\", \"wb\") as f:\n",
    "            pickle.dump(data_letter, f)\n",
    "\n",
    "    ari = [np.array(i['adjusted_rand_score']) for i in data_letter]\n",
    "    plotter(file_name=\"ari_letters/\"+model_name, model=model_name, data=ari, title=title,\n",
    "            ylabel=\"ARI\", yticks=0.5, avg=0)\n",
    "    plotter(file_name=\"ari_letters/\"+model_name, model=model_name, data=ari, title=title,\n",
    "            ylabel=\"ARI\", yticks=0.5, avg=AVG)\n",
    "    print(\"Clustering Letters done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusters Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following analysis we need to identify core points for each cluster, so we recompute the clustering with `halo==True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"clusters_analysis\" in _METRICS:\n",
    "    print(\"Clusters analysis\")\n",
    "    shot = [0, 1, 2, find_num_shot(str(_PATH))]\n",
    "    data_subjects_halo = []\n",
    "    cache_path = Path(f\"cache/{model_name}/\")\n",
    "    cache_path.mkdir(parents=True, exist_ok=True)\n",
    "    if os.path.exists(cache_path / \"subject_halo.pkl\"):\n",
    "        print(\"Loading from cache\")\n",
    "        with open(cache_path / \"subject_halo.pkl\", \"rb\") as f:\n",
    "            data_subjects_halo = pickle.load(f)\n",
    "    else:\n",
    "        for i in shot:\n",
    "            clustering = LabelClustering()\n",
    "            out_from_storage = retrieve_from_storage(_PATH / f'{i}shot',\n",
    "                                                     full_tensor=True,\n",
    "                                                     instances_per_sub=200)\n",
    "            tensors, labels, number_of_layers = out_from_storage\n",
    "            data_subjects_halo.append(clustering.main(\n",
    "                z=1.68,\n",
    "                tensors=tensors,\n",
    "                labels=labels[\"subjects\"],\n",
    "                halo=True,\n",
    "                number_of_layers=number_of_layers,\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        with open(cache_path / \"subject_halo.pkl\", \"wb\") as f:\n",
    "            pickle.dump(data_subjects_halo, f)\n",
    "\n",
    "        clustering = LabelClustering()\n",
    "        out_from_storage = retrieve_from_storage(_PATH_ft,\n",
    "                                                 full_tensor=True)\n",
    "        tensors, labels, number_of_layers = out_from_storage\n",
    "        data_subjects_halo.append(clustering.main(\n",
    "            z=1.68,\n",
    "            tensors=tensors,\n",
    "            labels=labels[\"subjects\"],\n",
    "            halo=True,      \n",
    "            number_of_layers=number_of_layers,\n",
    "            )\n",
    "        )\n",
    "        with open(cache_path / \"subject_halo.pkl\", \"wb\") as f:\n",
    "            pickle.dump(data_subjects_halo, f)\n",
    "    ari = [np.array(i['adjusted_rand_score']) for i in data_subjects]\n",
    "\n",
    "    label_clustering = LabelClustering()\n",
    "    \n",
    "    # set env variable\n",
    "    os.environ[\"OPENBLAS_NUM_THREADS\"] = \"128\"\n",
    "    metrics_subject = label_clustering.compute_additional_metrics(\n",
    "        data_subjects_halo\n",
    "        )\n",
    "\n",
    "    num_clusters = metrics_subject[\"num_clusters\"].to_list()\n",
    "    plotter(file_name=\"number_clusters/\"+model_name+\"-num-cluster\", model=model_name, data=num_clusters,\n",
    "            title=title, ylabel=\"Number of Clusters\", avg=0, yticks=[15, 80])\n",
    "    plotter(file_name=\"number_clusters/\"+model_name+\"-num-cluster\", model=model_name, data=num_clusters,\n",
    "            title=title, ylabel=\"Number of Clusters\", avg=AVG, yticks=[15, 80])\n",
    "\n",
    "    num_clusters = metrics_subject[\"num_assigned_points\"].to_list()\n",
    "    plotter(file_name=\"core_points/\"+model_name+\"-num-ass-point\", model=model_name, data=num_clusters,\n",
    "            title=title, ylabel=\"Core Point Fraction\", avg=0, yticks=[0.1, 0.65])\n",
    "    plotter(file_name=\"core_points/\"+model_name+\"-num-ass-point\", model=model_name, data=num_clusters,\n",
    "            title=title, ylabel=\"Core Point Fraction\", avg=AVG, yticks=[0.1, 0.65])\n",
    "    print(\"Clusters analysis done\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
